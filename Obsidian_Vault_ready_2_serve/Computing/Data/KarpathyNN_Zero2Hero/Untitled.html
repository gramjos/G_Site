<p>YT Video: https://www.youtube.com/watch?v=VMj-3S1tku0</p>
<h4><code class="inline-code">micrograd</code> is an automatic gradient engine(back prop)</h4>
<p><em>Back Propagation</em> efficient evaluate the a gradient of a loss function with respect to the weight of a Neural Net (NN). Therefore, one can iteratively tune the weights to improve accuracy.</p>
<h4><code class="inline-code">micrograd</code> allows one to build mathematical expressions</h4>
<p>Aside, Rectified Linear Unit <code class="inline-code">.relu()</code>. Informally described as <em>keep or squash to zero</em>. Can otherwise be stated as, <code class="inline-code">relu = max(0, x)</code>, and with the latex expression.</p>
<p>$$\text{relu }= \begin{cases} 0 & \text{if } x < 0 \\ x & \text{if } x \ge 0 \end{cases}$$</p>
<p><strong>Jargon</strong>: "Forward pass," meaning the series of expression calculated before the <code class="inline-code">.backward()</code> call (initialize back prop at node <code class="inline-code">g</code>)</p>
<div class="code-wrapper">
    <button class="copy-btn">Copy Code</button>
    <pre><code class="language-python">from micrograd.engine import Value

a = Value(-4.0)
b = Value(2.0)
c = a + b
d = a * b + b**3
c += c + 1
c += 1 + c + (-a)
d += d * 2 + (b + a).relu()
d += 3 * d + (b - a).relu()
e = c - d
f = e**2
g = f / 2.0
g += 10.0 / f
print(f&#39;{g.data:.4f}&#39;) # prints 24.7041, the outcome of this forward pass
g.backward()
print(f&#39;{a.grad:.4f}&#39;) # prints 138.8338, i.e. the numerical value of dg/da
print(f&#39;{b.grad:.4f}&#39;) # prints 645.5773, i.e. the numerical value of dg/db</code></pre>
</div>
<p>At node <code class="inline-code">g</code>, recursively, go through the expression graph to apply the chain rule from calculus. Every edge between nodes has a gradient, <code class="inline-code">a.grad</code>. <code class="inline-code">a.grad</code> how <code class="inline-code">a</code> is affecting <code class="inline-code">g</code>.</p>
<img src="hypothesis_node_graph_backprop.excalidraw|center" alt="" loading="lazy" decoding="async" />
<h4>Interpretation</h4>
<p><em>How does <code class="inline-code">g</code> respond when <code class="inline-code">a</code> is tweaked?</em></p>
<p>If we make <code class="inline-code">a</code> slightly larger then because <code class="inline-code">138.8338</code> is positive <code class="inline-code">g</code> will grow and the slope of that growth is <code class="inline-code">138.8338</code>.</p>
<h2>Zooming Out</h2>
<p><code class="inline-code">micrograd</code> is a scalar value AutoGrad Engine. This is a pedagogical and in practice these scalars would n-dimensional tensors</p>