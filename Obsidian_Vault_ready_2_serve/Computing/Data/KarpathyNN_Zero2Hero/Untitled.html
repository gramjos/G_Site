<p>YT Video: https://www.youtube.com/watch?v=VMj-3S1tku0</p>
<h4>`micrograd` is an automatic gradient engine(back prop)</h4>
<p>_Back Propagation_ efficient evaluate the a gradient of a loss function with respect to the weight of a Neural Net (NN). Therefore, one can iteratively tune the weights to improve accuracy.</p>
<h4>`micrograd` allows one to build mathematical expressions</h4>
<p>Aside, Rectified Linear Unit `.relu()`. Informally described as _keep or squash to zero_. Can otherwise be stated as, `relu = max(0, x)`, and with the latex expression.</p>
<p>$$\text{relu }= \begin{cases} 0 & \text{if } x < 0 \\ x & \text{if } x \ge 0 \end{cases}$$</p>
<p>__Jargon__: "Forward pass," meaning the series of expression calculated before the `.backward()` call (initialize back prop at node `g`)</p>
<div class="code-wrapper">
    <button class="copy-btn">Copy Code</button>
    <pre><code class="language-python">from micrograd.engine import Value

a = Value(-4.0)
b = Value(2.0)
c = a + b
d = a * b + b**3
c += c + 1
c += 1 + c + (-a)
d += d * 2 + (b + a).relu()
d += 3 * d + (b - a).relu()
e = c - d
f = e**2
g = f / 2.0
g += 10.0 / f
print(f&#39;{g.data:.4f}&#39;) # prints 24.7041, the outcome of this forward pass
g.backward()
print(f&#39;{a.grad:.4f}&#39;) # prints 138.8338, i.e. the numerical value of dg/da
print(f&#39;{b.grad:.4f}&#39;) # prints 645.5773, i.e. the numerical value of dg/db</code></pre>
</div>
<p>At node `g`, recursively, go through the expression graph to apply the chain rule from calculus. Every edge between nodes has a gradient, `a.grad`. `a.grad` how `a` is affecting `g`.</p>
<img src="hypothesis_node_graph_backprop.excalidraw|center" alt="" loading="lazy" decoding="async" />
<h4>Interpretation</h4>
<p>_How does `g` respond when `a` is tweaked?_</p>
<p>If we make `a` slightly larger then because `138.8338` is positive `g` will grow and the slope of that growth is `138.8338`.</p>
<h2>Zooming Out</h2>
<p>`micrograd` is a scalar value AutoGrad Engine. This is a pedagogical and in practice these scalars would n-dimensional tensors</p>